import os
import glob
import json
from time import perf_counter
from collections import Counter
import random
import math
from typing import Tuple, List, Dict
import numpy as np

# utils ë° config íŒŒì¼ ê²½ë¡œëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€ (ì™¸ë¶€ íŒŒì¼ë¡œ ê°€ì •)
from utils import log_message
from preprocessing.config import (
    USE_LABELS_LIST_PATH, FPS, RAW_DIR,
    ALL_MORPHEME_DIRS, SEQUENCE_LENGTH, SEQUENCE_STEP,
    TRAIN_TEST_SPLIT, LANDMARKS_DIR,
    TRAIN_FEATURES_DIR, TRAIN_LABELS_DIR,
    TEST_FEATURES_DIR, TEST_LABELS_DIR
)

# ----------------------------
# 0. WORD ë²ˆí˜¸ì™€ REAL ë²ˆí˜¸ì— ë”°ë¥¸ NPY í´ë” ê²°ì • í•¨ìˆ˜
# ----------------------------
def get_npy_folder_from_metadata(word_num: int, real_num_str: str) -> str:
    """WORD ë²ˆí˜¸ ë²”ìœ„ì™€ REAL ë²ˆí˜¸ì— ë”°ë¼ NPY íŒŒì¼ì´ ì €ì¥ëœ í´ë” ë²ˆí˜¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
    
    # 0~1500ì´ë©´ REAL ë²ˆí˜¸ ë’¤ì— -1ì´ ë¶™ìŒ
    if 0 <= word_num <= 1500:
        return f"{real_num_str}-1"
    
    # 1501 ì´ìƒì´ë©´ REAL ë²ˆí˜¸ë§Œ ì‚¬ìš©
    elif word_num > 1500:
        return real_num_str
    
    # ê·¸ ì™¸ì˜ ê²½ìš° (ìŒìˆ˜ ë“±)
    else:
        return "99_UNKNOWN_RANGE" 

# ----------------------------
# 1. ë¼ë²¨ ë§¤í•‘
# ----------------------------
def load_label_mapping():
    """ì‚¬ìš©í•  ë¼ë²¨ ëª©ë¡ì„ ë¶ˆëŸ¬ì˜¤ê³  ì •ìˆ˜í˜• ë§µí•‘ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    with open(USE_LABELS_LIST_PATH, "r", encoding="utf-8") as f:
        use_labels_list = [line.strip() for line in f if line.strip()]
    label_to_int = {label: i for i, label in enumerate(use_labels_list)}
    log_message(f"âœ… Loaded {len(use_labels_list)} labels from {os.path.basename(USE_LABELS_LIST_PATH)}")
    return label_to_int

# ----------------------------
# 2. JSON ìŠ¤ìº” ë° ë°ì´í„° ìˆ˜ì§‘ (NPY ê²½ë¡œ ìµœì¢… ìˆ˜ì • ì ìš©)
# ----------------------------
def scan_and_filter_data(label_to_int: dict):
    """ë¼ë²¨ JSON íŒŒì¼ì„ ìŠ¤ìº”í•˜ê³ , ìœ íš¨í•œ ìˆ˜ì–´ êµ¬ê°„ì„ ì¶”ì¶œí•˜ë©°, ë°ì´í„°ë¥¼ ê· í˜•í™”í•©ë‹ˆë‹¤."""
    max_morpheme_frames = 0
    all_label_data = []

    total_morpheme_dirs = len(ALL_MORPHEME_DIRS)
    log_message(f"ğŸ” Starting scan across {total_morpheme_dirs} morpheme directories...")
    
    # ë””ë²„ê¹…: NPY íŒŒì¼ ëˆ„ë½ ë¼ë²¨ ê¸°ë¡ìš©
    missing_npy_files_counter = Counter()

    for i, morpheme_dir_with_speaker in enumerate(ALL_MORPHEME_DIRS):
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if (i + 1) % 50 == 0 or (i + 1) == total_morpheme_dirs:
            log_message(f"   ... Scanning progress: {i + 1}/{total_morpheme_dirs} directories processed.")
            
        label_files = glob.glob(os.path.join(morpheme_dir_with_speaker, "**", "*.json"), recursive=True)
        for label_path in label_files:
            try:
                with open(label_path, "r", encoding="utf-8") as f:
                    full_json_data = json.load(f)
            except Exception as e:
                log_message(f"[WARN] Failed to load JSON {label_path}: {e}")
                continue

            sign_segments = full_json_data.get("data", [])
            if not sign_segments:
                continue

            for segment in sign_segments:
                if not segment.get("attributes"):
                    continue
                word = segment["attributes"][0].get("name")
                
                # 1. ë¼ë²¨ ë¶ˆì¼ì¹˜ ì²´í¬
                if not word or word not in label_to_int:
                    continue

                start_frame = int(segment.get("start", 0) * FPS)
                end_frame = int(segment.get("end", 0) * FPS)
                morpheme_frames = end_frame - start_frame + 1
                if morpheme_frames <= 0:
                    continue

                max_morpheme_frames = max(max_morpheme_frames, morpheme_frames)

                # npy íŒŒì¼ ê²½ë¡œ ë§¤í•‘
                json_name = os.path.basename(label_path)
                base_name = json_name.replace("_morpheme.json", "")
                
                # ----------------------------------------------------
                # â­ NPY íŒŒì¼ ê²½ë¡œ ë§¤í•‘ ë¡œì§ â­
                # ----------------------------------------------------
                # 1. WORD ë²ˆí˜¸ ì¶”ì¶œ
                try:
                    word_part = base_name.split("_")[2] 
                    word_num = int(word_part.replace("WORD", ""))
                except (IndexError, ValueError):
                    log_message(f"[WARN] Failed to parse WORD num from {base_name}")
                    continue
                
                # 2. REAL ë²ˆí˜¸ ì¶”ì¶œ
                try:
                    real_num_str = base_name.split("_REAL")[1].split("_")[0]
                except (IndexError, ValueError):
                    log_message(f"[WARN] Failed to parse REAL num from {base_name}")
                    continue

                # 3. WORD ë²ˆí˜¸ì™€ REAL ë²ˆí˜¸ì— ë”°ë¼ NPY í´ë” ê²°ì •
                npy_root_folder = get_npy_folder_from_metadata(word_num, real_num_str)
                
                # 4. NPY ë””ë ‰í† ë¦¬ ê²½ë¡œ êµ¬ì„±
                npy_dir = os.path.join(LANDMARKS_DIR, npy_root_folder, base_name)
                    
                # ----------------------------------------------------

                # 5. NPY íŒŒì¼ ì¡´ì¬ ì—¬ë¶€ ì²´í¬
                npy_files = sorted(glob.glob(os.path.join(npy_dir, "*.npy")))
                
                if not npy_files:
                    # â­ ë””ë²„ê¹… ë¡œì§: NPY íŒŒì¼ì´ ì—†ëŠ” ê²½ìš° ê²½ë¡œ ì¶œë ¥ â­
                    missing_npy_files_counter[word] += 1
                    log_message(f"[DEBUG_NPY_MISSING] Word: {word}, Expected NPY Dir: {npy_dir}, JSON Path: {label_path}")
                    continue

                all_label_data.append({
                    "word": word,
                    "int_label": label_to_int[word],
                    "start_frame": start_frame,
                    "end_frame": min(end_frame, len(npy_files)-1),
                    "npy_files": npy_files
                })

    # ì–¸ë”ìƒ˜í”Œë§ & ë°¸ëŸ°ì‹±
    data_by_label = {}
    for item in all_label_data:
        word = item['word']
        data_by_label.setdefault(word, []).append(item)

    if not data_by_label:
        log_message("âŒ No valid data found for any label.")
        return 0, [], {} # max_len, balanced_data, data_by_label ë°˜í™˜ êµ¬ì¡° ë³€ê²½ ê³ ë ¤
    
    # ìŠ¤ìº” í›„ ì›ë³¸ ë°ì´í„° ê°œìˆ˜ ì •ë³´ ë°˜í™˜
    original_data_counts = {word: len(items) for word, items in data_by_label.items()}

    # ë””ë²„ê¹…: NPY íŒŒì¼ ëˆ„ë½ ìš”ì•½ ì¶œë ¥
    if missing_npy_files_counter:
        log_message("--- Summary of Missing NPY Files by Word ---")
        for word, count in missing_npy_files_counter.most_common():
            # ìµœì¢… ë°ì´í„°ì— ë‚¨ì§€ ì•Šì€ ë¼ë²¨ë§Œ ì¶œë ¥í•˜ì—¬ ë¬¸ì œ ë¼ë²¨ì— ì§‘ì¤‘
            if word not in data_by_label:
                 log_message(f"  > {word}: {count} occurrences missing NPY files.")
        log_message("--------------------------------------------")
        
    min_count = min(len(v) for v in data_by_label.values())
    balanced_data = []
    
    log_message(f"ğŸ“Š Original total samples: {len(all_label_data)}")
    log_message(f"âš–ï¸ Balancing data. Minimum samples per label: {min_count}")
    
    for word, items in data_by_label.items():
        random.shuffle(items)
        balanced_data.extend(items[:min_count])
        
    log_message(f"âœ… Scanning and balancing finished. Final balanced samples: {len(balanced_data)}")
    return max_morpheme_frames, balanced_data

# ----------------------------
# 3. í”„ë ˆì„ ì •ê·œí™”
# ----------------------------
def normalize_frames(data_list: list, max_frames: int):
    """ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ë¥¼ ê°€ì¥ ê¸´ ê¸¸ì´(max_frames)ì— ë§ê²Œ í”„ë ˆì„ì„ í™•ì¥í•©ë‹ˆë‹¤."""
    log_message(f"ğŸ“ Normalizing sequence lengths to {max_frames} frames.")
    for item in data_list:
        start, end = item['start_frame'], item['end_frame']
        current_length = end - start + 1
        
        # ì—¬ê¸°ì„œ 'ì‹œì‘/ëì„ ë™ì¼í•˜ê²Œ ëŠ˜ë¦¬ê¸°' ì „ëµ ëŒ€ì‹ ,
        # ìµœëŒ€ ê¸¸ì´ë¡œ ë§ì¶”ê¸° ìœ„í•œ padding ê³„ì‚°ë§Œ ìˆ˜í–‰ (ë³´ê°„ ì—†ì´)
        padding_needed = max_frames - current_length
        pad_start = math.floor(padding_needed / 2)
        
        # 0 í”„ë ˆì„ë³´ë‹¤ ì‘ì•„ì§€ì§€ ì•Šë„ë¡ ì¡°ì •
        new_start = max(start - pad_start, 0)
        # new_endëŠ” L_max ê¸¸ì´ë§Œí¼ ë³´ì¥í•˜ë„ë¡ ê³„ì‚°
        new_end = new_start + max_frames - 1
        
        # ì›ë³¸ npy_files ê¸¸ì´ë¥¼ ë„˜ì§€ ì•Šë„ë¡ ì¡°ì •
        max_valid_frame = len(item['npy_files']) - 1
        if new_end > max_valid_frame:
             new_end = max_valid_frame
             new_start = new_end - max_frames + 1
             new_start = max(new_start, 0) # ë‹¤ì‹œ 0ë³´ë‹¤ ì‘ì•„ì§€ì§€ ì•Šë„ë¡ í™•ì¸

        item['normalized_start_frame'] = new_start
        item['normalized_end_frame'] = new_end
        
    log_message("âœ… Normalization complete.")
    return data_list

# ----------------------------
# 4. ìŠ¬ë¼ì´ë”© ìœˆë„ìš°
# ----------------------------
def create_sliding_windows(data_list: List[Dict], sequence_length: int, sequence_step: int) -> List[Dict]:
    """ì •ê·œí™”ëœ ì‹œí€€ìŠ¤ë¥¼ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ë°©ì‹ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
    sequences = []
    seq_id = 0
    for item in data_list:
        norm_start, norm_end = item['normalized_start_frame'], item['normalized_end_frame']
        # ì‹œì‘ í”„ë ˆì„ ê³„ì‚° (end_frame - sequence_length + 1 ê¹Œì§€ í¬í•¨)
        for start_frame in range(norm_start, norm_end - sequence_length + 2, sequence_step): 
            end_frame = start_frame + sequence_length - 1
            
            # ìœˆë„ìš°ê°€ ì •ê·œí™”ëœ ì‹œí€€ìŠ¤ ë²”ìœ„ ì•ˆì— ì™„ì „íˆ ë“¤ì–´ì˜¤ëŠ”ì§€ í™•ì¸
            if end_frame <= norm_end: 
                seq_item = item.copy()
                seq_item.update({
                    "sequence_id": seq_id,
                    "sequence_start_frame": start_frame,
                    "sequence_end_frame": end_frame,
                    "sequence_length": sequence_length
                })
                sequences.append(seq_item)
                seq_id += 1
    return sequences

# ----------------------------
# 5. ì¢Œí‘œ ë³€í™˜ ë° ì •ê·œí™”
# ----------------------------
def transform_and_normalize_landmarks(landmarks_array: np.ndarray) -> np.ndarray:
    """í”„ë ˆì„ë³„ ëœë“œë§ˆí¬ë¥¼ ì†ëª© ê¸°ì¤€ìœ¼ë¡œ ë³€í™˜í•˜ê³  ì •ê·œí™”í•©ë‹ˆë‹¤."""
    if landmarks_array.size == 0:
        return landmarks_array
        
    # ì†ëª©(0ë²ˆ ì¸ë±ìŠ¤)ì„ ê¸°ì¤€ì (0, 0, 0)ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ìƒëŒ€ ì¢Œí‘œë¡œ ë³€í™˜
    reference_point = landmarks_array[:, 0:1, :] # (T, 1, 3)
    transformed = landmarks_array - reference_point
    
    # ì† í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì •ê·œí™” (ìµœëŒ€ ìœ í´ë¦¬ë“œ ê±°ë¦¬ë¡œ ë‚˜ëˆ”)
    max_norm = np.max(np.linalg.norm(transformed, axis=-1, keepdims=True))
    
    if max_norm < 1e-6:
        # 0ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê²ƒì„ ë°©ì§€
        return transformed
        
    return transformed / max_norm

# ----------------------------
# 6. ì‹œí€€ìŠ¤ ì €ì¥ (ì§„í–‰ ìƒí™© ì¶”ê°€)
# ----------------------------
def save_sequences(sequences: List[Dict], features_dir: str, labels_dir: str, name: str):
    """ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì‹œí€€ìŠ¤ë¥¼ .npy íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤."""
    os.makedirs(features_dir, exist_ok=True)
    os.makedirs(labels_dir, exist_ok=True)
    
    total_sequences = len(sequences)
    log_message(f"ğŸ’¾ Starting to save {name} sequences ({total_sequences} total) to {os.path.basename(features_dir)}/")

    for i, seq_item in enumerate(sequences):
        # ì§„í–‰ ìƒí™© ì¶œë ¥
        if (i + 1) % 1000 == 0 or (i + 1) == total_sequences:
            log_message(f"   ... Saving {name} sequences: {i + 1}/{total_sequences} processed.")
            
        npy_files = seq_item['npy_files']
        start, end = seq_item['sequence_start_frame'], seq_item['sequence_end_frame']
        seq_frames = []

        # ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ npy íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬
        for f in npy_files[start:end+1]:
            frame = np.load(f)

            # ë¹ˆ ë°°ì—´ ì²˜ë¦¬ (íŒ¨ë”©ìš©)
            if frame.size == 0:
                frame = np.zeros((1, 21, 3), dtype=np.float32)

            # ì°¨ì› ë§ì¶¤: (J, 3) -> (1, J, 3)
            if frame.ndim == 2:
                frame = frame[np.newaxis, :, :] 
            elif frame.ndim == 1:
                # 1ì°¨ì› ë°ì´í„°ê°€ ë“¤ì–´ì˜¬ ê²½ìš° (ë§¤ìš° ë“œë¬¼ì§€ë§Œ ì•ˆì „ì¥ì¹˜)
                frame = np.zeros((1, 21, 3), dtype=np.float32)

            # ì† ëœë“œë§ˆí¬ 21ê°œ ë§ì¶¤ (íŒ¨ë”©)
            if frame.shape[1] < 21:
                pad = np.zeros((frame.shape[0], 21 - frame.shape[1], frame.shape[2]), dtype=frame.dtype)
                frame = np.concatenate([frame, pad], axis=1)

            seq_frames.append(frame)

        # ì‹œí€€ìŠ¤ ê²°í•© ë° ìµœì¢… ì •ê·œí™”
        seq_array = np.vstack(seq_frames)  # (T, J, 3)
        seq_array = transform_and_normalize_landmarks(seq_array)
        
        # LSTM ì…ë ¥ì— ë§ê²Œ 2ì°¨ì›ìœ¼ë¡œ í¼ì¹¨ (T, J*C) -> (SEQUENCE_LENGTH, 63)
        seq_flat = seq_array.reshape(seq_array.shape[0], -1) 

        feat_path = os.path.join(features_dir, f"{seq_item['word']}_{seq_item['sequence_id']}.npy")
        lbl_path = os.path.join(labels_dir, f"{seq_item['word']}_{seq_item['sequence_id']}.txt")

        # íŒŒì¼ ì €ì¥
        np.save(feat_path, seq_flat)
        with open(lbl_path, "w") as f:
            f.write(str(seq_item['int_label']))
            
    log_message(f"âœ… Saving {name} sequences complete.")

# ----------------------------
# 7. Train/Test split
# ----------------------------
def split_data_by_label(data_list: List[Dict], split_ratio: float = 0.8) -> Tuple[List[Dict], List[Dict]]:
    """ë¼ë²¨ë³„ë¡œ í›ˆë ¨/í…ŒìŠ¤íŠ¸ ì…‹ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤."""
    data_by_label = {}
    for item in data_list:
        word = item['word']
        data_by_label.setdefault(word, []).append(item)

    train_data, test_data = [], []
    random.seed(42) # ì¬í˜„ì„±ì„ ìœ„í•´ ì‹œë“œ ê³ ì •
    
    log_message(f"âœ‚ï¸ Splitting data by label with ratio: {split_ratio:.2f} (Train) / {1-split_ratio:.2f} (Test)")

    for word, items in data_by_label.items():
        random.shuffle(items)
        split_point = int(len(items) * split_ratio)
        
        train_data.extend(items[:split_point])
        test_data.extend(items[split_point:])
        
        log_message(f"   ... {word}: Train={len(items[:split_point])}, Test={len(items[split_point:])}")

    log_message(f"âœ… Split complete. Total Train meta: {len(train_data)}, Total Test meta: {len(test_data)}")
    return train_data, test_data

# ----------------------------
# 9. ì‹œí€€ìŠ¤ ê¸¸ì´ í™•ì¸ ë° ë¡œê·¸ ì¶œë ¥
# ----------------------------
def check_sequence_lengths(sequences: List[Dict], name: str):
    """ëª¨ë“  ì‹œí€€ìŠ¤ì˜ ê¸¸ì´ê°€ ë™ì¼í•œì§€ í™•ì¸í•˜ê³ , ë‹¤ë¥¸ ê¸¸ì´ê°€ ìˆëŠ” ê²½ìš° ì¶œë ¥."""
    lengths = [seq['sequence_end_frame'] - seq['sequence_start_frame'] + 1 for seq in sequences]
    unique_lengths = set(lengths)
    
    if len(unique_lengths) == 1:
        log_message(f"âœ… All {name} sequences have consistent length: {unique_lengths.pop()} frames")
    else:
        log_message(f"[WARN] {name} sequences have inconsistent lengths: {sorted(unique_lengths)}")
        # ê¸¸ì´ë³„ ëª‡ ê°œì”© ìˆëŠ”ì§€ë„ ì¶œë ¥
        length_counts = Counter(lengths)
        for length, count in sorted(length_counts.items()):
            log_message(f"  > Length {length}: {count} sequences")

# ----------------------------
# 8. ì‹¤í–‰
# ----------------------------
if __name__ == "__main__":
    start_time = perf_counter()
    log_message("--- Start Data Pipeline ---")

    label_map = load_label_mapping()
    
    # ----------------------------------------------------
    # ë¼ë²¨ ë§¤í•‘ ë° ì´ˆê¸°í™”
    # ----------------------------------------------------
    log_message("--- Loaded Label Mapping ---")
    if label_map:
        sorted_labels = sorted(label_map.items(), key=lambda item: item[1])
        for label, index in sorted_labels:
            log_message(f"  > Index {index:2d}: {label}")
    log_message("----------------------------")
    
    # ----------------------------------------------------
    # ë°ì´í„° ìŠ¤ìº” ë° ì²˜ë¦¬
    # ----------------------------------------------------
    if label_map:
        max_len, balanced_data = scan_and_filter_data(label_map)
        
        if max_len > 0 and balanced_data:
            # 3. í”„ë ˆì„ ì •ê·œí™”
            final_data = normalize_frames(balanced_data, max_len)

            # 7. Train/Test split
            train_data_meta, test_data_meta = split_data_by_label(final_data, split_ratio=TRAIN_TEST_SPLIT)
            
            # 4. ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ìƒì„±
            log_message(f"ğŸ“ Creating sequences (Length={SEQUENCE_LENGTH}, Step={SEQUENCE_STEP})...")
            train_sequences = create_sliding_windows(train_data_meta, SEQUENCE_LENGTH, SEQUENCE_STEP)
            test_sequences = create_sliding_windows(test_data_meta, SEQUENCE_LENGTH, SEQUENCE_STEP)
            log_message(f"âœ… Sequence creation complete. Train sequences: {len(train_sequences)}, Test sequences: {len(test_sequences)}")

            # ----------------------------------------------------
            # â­ ë¼ë²¨ë³„ ìµœì¢… ì‹œí€€ìŠ¤ ê°œìˆ˜ ì¶œë ¥ (ì¶”ê°€ëœ ë¶€ë¶„) â­
            # ----------------------------------------------------
            train_counts = Counter(item['word'] for item in train_sequences)
            test_counts = Counter(item['word'] for item in test_sequences)
            
            log_message("\n--- Final Sequence Counts by Label ---")
            
            # ë¼ë²¨ ë§µí•‘ ìˆœì„œëŒ€ë¡œ ì¶œë ¥
            for label, index in sorted_labels:
                train_count = train_counts.get(label, 0)
                test_count = test_counts.get(label, 0)
                log_message(f"  > {label} (Idx {index:2d}): Train={train_count:,} sequences, Test={test_count:,} sequences")
            
            total_train_seq = sum(train_counts.values())
            total_test_seq = sum(test_counts.values())
            log_message(f"--- TOTAL: Train={total_train_seq:,} sequences, Test={total_test_seq:,} sequences ---")
            
            # ----------------------------------------------------
            
            # 6. ì‹œí€€ìŠ¤ ì €ì¥
            save_sequences(train_sequences, TRAIN_FEATURES_DIR, TRAIN_LABELS_DIR, name="TRAIN")
            save_sequences(test_sequences, TEST_FEATURES_DIR, TEST_LABELS_DIR, name="TEST")

            log_message(f"âœ… Final save complete. Total train sequences: {len(train_sequences)}, Total test sequences: {len(test_sequences)}.")
        else:
            log_message("[WARN] No valid data found after scanning and balancing.")
            
    log_message(f"--- Pipeline finished in {perf_counter() - start_time:.2f}s ---")
    
    check_sequence_lengths(train_sequences, "TRAIN")
    check_sequence_lengths(test_sequences, "TEST")