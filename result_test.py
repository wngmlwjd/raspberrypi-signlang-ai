import tensorflow as tf
import numpy as np
from pathlib import Path
from sklearn.metrics import accuracy_score, classification_report
import json
from sign_language_recognition.train.utils import log_message, MODEL_CHECKPOINT_PATH, X_NPY_PATH, Y_NPY_PATH, ENCODER_PATH
from sklearn.model_selection import train_test_split


def load_label_encoder_map(encoder_path: Path) -> dict:
    """
    ë ˆì´ë¸” ì¸ì½”ë” íŒŒì¼ì„ ë¡œë“œí•˜ê³ , ì¸ë±ìŠ¤(int)ë¥¼ ë‹¨ì–´(str)ë¡œ ë§¤í•‘í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    JSON íŒŒì¼ êµ¬ì¡°: {"classes": ["word1", "word2", ...]}
    """
    if not encoder_path.exists():
        log_message(f"Error: Label encoder file not found at {encoder_path}")
        return {}
    
    with open(encoder_path, 'r', encoding='utf-8') as f:
        try:
            # ì›ë³¸ JSON ë¡œë“œ: {"classes": [word, ...]} í˜•íƒœ
            data = json.load(f)
            
            # "classes" í‚¤ì—ì„œ ë‹¨ì–´ ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ
            classes = data.get('classes', [])
            
            # ì¸ë±ìŠ¤(0, 1, 2, ...)ë¥¼ í‚¤ë¡œ, ë‹¨ì–´ë¥¼ ê°’ìœ¼ë¡œ ë§¤í•‘
            index_to_label = {i: label for i, label in enumerate(classes)}
            
            return index_to_label
            
        except Exception as e:
            log_message(f"Error loading or parsing label encoder JSON: {repr(e)}")
            return {}


def test_sign_language_model(num_samples_to_test: int = -1, validation_split: float = 0.2):
    """
    Loads the trained model and evaluates its performance on the validation set.

    Args:
        num_samples_to_test (int): Number of samples to use for testing (-1 for all).
        validation_split (float): The validation split used during training (to replicate the split).
    """
    log_message("--- Model Test and Evaluation Start ---")

    # 1. Load Model Checkpoint
    if not MODEL_CHECKPOINT_PATH.exists():
        log_message(f"Error: Trained model not found at {MODEL_CHECKPOINT_PATH}. Cannot proceed with testing.")
        return

    try:
        model = tf.keras.models.load_model(str(MODEL_CHECKPOINT_PATH))
        log_message(f"Successfully loaded model from: {MODEL_CHECKPOINT_PATH.name}")
    except Exception as e:
        log_message(f"Error loading model: {repr(e)}")
        return

    # 2. Load Data and Label Encoder
    if not X_NPY_PATH.exists() or not Y_NPY_PATH.exists():
        log_message("Error: Processed data (x.npy or y.npy) not found. Run preprocessing first.")
        return

    try:
        X = np.load(X_NPY_PATH)
        Y = np.load(Y_NPY_PATH)
        log_message(f"Data loaded. X shape: {X.shape}, Y shape: {Y.shape}")
        
        index_to_label_map = load_label_encoder_map(ENCODER_PATH)
        
        # í´ëž˜ìŠ¤ ì´ë¦„ ë¦¬ìŠ¤íŠ¸ ìƒì„± (ì •ë ¬ëœ ì¸ë±ìŠ¤ ìˆœì„œëŒ€ë¡œ)
        sorted_indices = sorted(index_to_label_map.keys())
        class_names = [index_to_label_map.get(i, f"Class_{i}") for i in sorted_indices]
        
    except Exception as e:
        # ì´ì „ì— ë°œìƒí–ˆë˜ numpy ë¡œë“œ ì˜¤ë¥˜ë‚˜ ë‹¤ë¥¸ ì˜ˆì™¸ë¥¼ ì²˜ë¦¬
        log_message(f"Error loading numpy data or encoder: {repr(e)}")
        return

    # 3. Data Split (Replicating Training Split)
    # í›ˆë ¨ ì‹œ ì‚¬ìš©í•œ random_state=42ì™€ stratify=Yë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•´ì•¼ í•©ë‹ˆë‹¤.
    _, X_val, _, Y_val = train_test_split(
        X, Y, test_size=validation_split, shuffle=True, random_state=42, stratify=Y
    )
    log_message(f"Validation dataset size: {X_val.shape[0]} samples.")
    
    # 4. Determine Test Samples
    if num_samples_to_test > 0 and num_samples_to_test < X_val.shape[0]:
        X_test = X_val[:num_samples_to_test]
        Y_true = Y_val[:num_samples_to_test]
        log_message(f"Using a subset of {num_samples_to_test} samples for testing.")
    else:
        X_test = X_val
        Y_true = Y_val
        log_message(f"Using the entire validation set ({X_test.shape[0]} samples) for testing.")

    # 5. Model Prediction
    log_message("Starting prediction...")
    Y_pred_proba = model.predict(X_test, verbose=0)
    Y_pred = np.argmax(Y_pred_proba, axis=1)

    # 6. Evaluation and Reporting
    
    # Calculate Overall Accuracy
    accuracy = accuracy_score(Y_true, Y_pred)
    log_message(f"\n--- Overall Test Accuracy: {accuracy * 100:.2f}% ---")

    # Generate Classification Report (Precision, Recall, F1-Score)
    # í´ëž˜ìŠ¤ ê°œìˆ˜ê°€ ë„ˆë¬´ ë§Žì§€ ì•Šì€ ê²½ìš°ì—ë§Œ ìƒì„¸ ë¦¬í¬íŠ¸ ì¶œë ¥
    unique_true_classes = np.unique(Y_true)
    if len(unique_true_classes) > 1 and len(unique_true_classes) < 50 and class_names:
        # unique_true_classesì— í•´ë‹¹í•˜ëŠ” ì´ë¦„ë§Œ í•„í„°ë§í•˜ì—¬ ì‚¬ìš©
        # class_namesëŠ” ì „ì²´ ì¸ë±ìŠ¤(0 ~ max_index)ì— ëŒ€í•œ ë‹¨ì–´ëª…ì„ ë‹´ê³  ìžˆìŠµë‹ˆë‹¤.
        # Y_trueëŠ” ì¸ë±ìŠ¤ì´ë¯€ë¡œ, Y_trueì— ìžˆëŠ” ì¸ë±ìŠ¤ë§Œ ì‚¬ìš©í•˜ì—¬ target_namesë¥¼ êµ¬ì„±í•©ë‹ˆë‹¤.
        target_names = [index_to_label_map.get(i, f"Class_{i}") for i in unique_true_classes]
        log_message("\n--- Detailed Classification Report ---")
        print(classification_report(
            Y_true, 
            Y_pred, 
            labels=unique_true_classes, # <--- ðŸ’¡ Y_trueì— í¬í•¨ëœ ì‹¤ì œ ì¸ë±ìŠ¤ ë¦¬ìŠ¤íŠ¸
            target_names=target_names,   # <--- ðŸ’¡ ì´ ì¸ë±ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ì´ë¦„ ë¦¬ìŠ¤íŠ¸
            zero_division=0
        ))
    else:
        log_message(f"\nDetailed Classification Report skipped (Classes in test set: {len(unique_true_classes)}).")


if __name__ == '__main__':
    # ê²€ì¦ ë°ì´í„° ì¤‘ ì²˜ìŒ 10ê°œ ìƒ˜í”Œë¡œ í…ŒìŠ¤íŠ¸:
    test_sign_language_model(num_samples_to_test=10)
    
    log_message("--- Model Test Complete ---")
